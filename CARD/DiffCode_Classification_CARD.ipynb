{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28158,"status":"ok","timestamp":1724913626915,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"jjbIqP_u91tf","outputId":"9f47ce20-9628-435e-f5f1-7e14bd5d9e66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"YsW8atyq_KNH"},"source":["# Data"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6221,"status":"ok","timestamp":1724913633132,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"owSRngeA_Ozk","outputId":"cb289d7e-59d3-4e1a-ff13-7be21b715ce5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.43)\n","Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.1.4)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.26.4)\n","Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n","Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n","Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n","Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.1)\n","Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n","Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.6)\n","Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n","Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.7.4)\n","Collecting pandas_ta\n","  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas_ta) (2.1.4)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas_ta) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.16.0)\n","Building wheels for collected packages: pandas_ta\n","  Building wheel for pandas_ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218909 sha256=2b8a71c810952661161fb6d7d979cc8b6117c759d08ebb35b6b47b033db030fd\n","  Stored in directory: /root/.cache/pip/wheels/69/00/ac/f7fa862c34b0e2ef320175100c233377b4c558944f12474cf0\n","Successfully built pandas_ta\n","Installing collected packages: pandas_ta\n","Successfully installed pandas_ta-0.3.14b0\n"]}],"source":["!pip install yfinance\n","!pip install pandas_ta"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8792,"status":"ok","timestamp":1724913641919,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"vkgPi20c_Q42"},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math, copy, time\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","import seaborn\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import Dataset\n","import tqdm\n","import yfinance as yf\n","from torch.utils.data import DataLoader\n","from torch.distributions import Categorical\n","import tqdm\n","from google.colab import runtime\n","# Finance Data\n","import pandas as pd\n","import pandas_ta as ta\n","from typing import List\n","\n","import numpy as np\n","import pandas as pd\n","from pandas.tseries import offsets\n","from pandas.tseries.frequencies import to_offset\n","from torch.utils.data import DataLoader\n","\n","\n","seaborn.set_context(context=\"talk\")\n","%matplotlib inline"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1724913641920,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"lm9r2rIQFA4o"},"outputs":[],"source":["class TimeFeature:\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        pass\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + \"()\"\n","\n","\n","class SecondOfMinute(TimeFeature):\n","    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return index.second / 59.0 - 0.5\n","\n","\n","class MinuteOfHour(TimeFeature):\n","    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return index.minute / 59.0 - 0.5\n","\n","\n","class HourOfDay(TimeFeature):\n","    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return index.hour / 23.0 - 0.5\n","\n","\n","class DayOfWeek(TimeFeature):\n","    \"\"\"Day of week encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return index.dayofweek / 6.0 - 0.5\n","\n","\n","class DayOfMonth(TimeFeature):\n","    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return (index.day - 1) / 30.0 - 0.5\n","\n","\n","class DayOfYear(TimeFeature):\n","    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return (index.dayofyear - 1) / 365.0 - 0.5\n","\n","\n","class MonthOfYear(TimeFeature):\n","    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return (index.month - 1) / 11.0 - 0.5\n","\n","\n","class WeekOfYear(TimeFeature):\n","    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n","\n","    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n","        return (index.isocalendar().week - 1) / 52.0 - 0.5\n","\n","\n","def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n","    \"\"\"\n","    Returns a list of time features that will be appropriate for the given frequency string.\n","    Parameters\n","    ----------\n","    freq_str\n","        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n","    \"\"\"\n","    #print(f\"freq_str: {freq_str}\")\n","\n","    features_by_offsets = {\n","        offsets.YearEnd: [],\n","        offsets.QuarterEnd: [MonthOfYear],\n","        offsets.MonthEnd: [MonthOfYear],\n","        offsets.Week: [DayOfMonth, WeekOfYear],\n","        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n","        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n","        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n","        offsets.Minute: [\n","            MinuteOfHour,\n","            HourOfDay,\n","            DayOfWeek,\n","            DayOfMonth,\n","            DayOfYear,\n","        ],\n","        offsets.Second: [\n","            SecondOfMinute,\n","            MinuteOfHour,\n","            HourOfDay,\n","            DayOfWeek,\n","            DayOfMonth,\n","            DayOfYear,\n","        ],\n","    }\n","\n","    offset = to_offset(freq_str)\n","    #print(offset)\n","\n","    for offset_type, feature_classes in features_by_offsets.items():\n","        #print(f\"offset_type: {offset_type}\")\n","        if isinstance(offset, offset_type):\n","            #print(cls for cls in feature_classes)\n","            return [cls() for cls in feature_classes]\n","\n","    supported_freq_msg = f\"\"\"\n","    Unsupported frequency {freq_str}\n","    The following frequencies are supported:\n","        Y   - yearly\n","            alias: A\n","        M   - monthly\n","        W   - weekly\n","        D   - daily\n","        B   - business days\n","        H   - hourly\n","        T   - minutely\n","            alias: min\n","        S   - secondly\n","    \"\"\"\n","    raise RuntimeError(supported_freq_msg)\n","\n","\n","def time_features(dates, freq='h'):\n","    #print(f\"Entered time_features!!\")\n","    #print(dates)\n","    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":995},"executionInfo":{"elapsed":1482,"status":"ok","timestamp":1724913643395,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"yWzbrmlQ_aiq","outputId":"7cfcfe86-eb05-42d3-d887-44568a93cd20"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                  Open         High          Low        Close  \\\n","Date                                                                            \n","1989-01-03 00:00:00-05:00   277.720001   277.720001   273.809998   275.309998   \n","1989-01-04 00:00:00-05:00   275.309998   279.750000   275.309998   279.429993   \n","1989-01-05 00:00:00-05:00   279.429993   281.510010   279.429993   280.010010   \n","1989-01-06 00:00:00-05:00   280.010010   282.059998   280.010010   280.670013   \n","1989-01-09 00:00:00-05:00   280.670013   281.890015   280.320007   280.980011   \n","...                                ...          ...          ...          ...   \n","2023-12-22 00:00:00-05:00  4753.919922  4772.939941  4736.770020  4754.629883   \n","2023-12-26 00:00:00-05:00  4758.859863  4784.720215  4758.450195  4774.750000   \n","2023-12-27 00:00:00-05:00  4773.450195  4785.390137  4768.899902  4781.580078   \n","2023-12-28 00:00:00-05:00  4786.439941  4793.299805  4780.979980  4783.350098   \n","2023-12-29 00:00:00-05:00  4782.879883  4788.430176  4751.990234  4769.830078   \n","\n","                                SMA_50      SMA_200     RSI_14    BBL_5_2.0  \\\n","Date                                                                          \n","1989-01-03 00:00:00-05:00   274.762200   268.111900  48.838065   274.613084   \n","1989-01-04 00:00:00-05:00   274.677599   268.165350  58.288271   274.697225   \n","1989-01-05 00:00:00-05:00   274.632200   268.221200  59.424554   274.949366   \n","1989-01-06 00:00:00-05:00   274.598000   268.280000  60.735306   274.775049   \n","1989-01-09 00:00:00-05:00   274.590000   268.368150  61.366579   275.167730   \n","...                                ...          ...        ...          ...   \n","2023-12-22 00:00:00-05:00  4468.218584  4335.861046  71.069402  4694.534676   \n","2023-12-26 00:00:00-05:00  4477.157988  4340.426846  72.704374  4694.610700   \n","2023-12-27 00:00:00-05:00  4485.316992  4345.055946  73.256889  4692.545294   \n","2023-12-28 00:00:00-05:00  4493.519990  4349.376246  73.407118  4738.605198   \n","2023-12-29 00:00:00-05:00  4502.624590  4353.765747  70.164874  4752.214119   \n","\n","                             BBM_5_2.0    BBU_5_2.0  ...  SMIo_5_20_5  \\\n","Date                                                 ...                \n","1989-01-03 00:00:00-05:00   277.267993   279.922903  ...    -0.057734   \n","1989-01-04 00:00:00-05:00   277.787994   280.878764  ...    -0.011009   \n","1989-01-05 00:00:00-05:00   278.373999   281.798632  ...     0.014795   \n","1989-01-06 00:00:00-05:00   278.628003   282.480957  ...     0.030888   \n","1989-01-09 00:00:00-05:00   279.280005   283.392280  ...     0.037330   \n","...                                ...          ...  ...          ...   \n","2023-12-22 00:00:00-05:00  4741.732031  4788.929386  ...    -0.054860   \n","2023-12-26 00:00:00-05:00  4748.570020  4802.529339  ...    -0.035086   \n","2023-12-27 00:00:00-05:00  4751.212012  4809.878729  ...    -0.019366   \n","2023-12-28 00:00:00-05:00  4768.212012  4797.818826  ...    -0.009495   \n","2023-12-29 00:00:00-05:00  4772.828027  4793.441935  ...    -0.022409   \n","\n","                            WILLR_14  STOCHk_14_3_3  STOCHd_14_3_3  \\\n","Date                                                                 \n","1989-01-03 00:00:00-05:00 -77.409688      54.631409      58.669202   \n","1989-01-04 00:00:00-05:00 -15.361706      54.945716      57.522858   \n","1989-01-05 00:00:00-05:00 -19.480489      62.582706      57.386610   \n","1989-01-06 00:00:00-05:00 -16.848292      82.769838      66.766086   \n","1989-01-09 00:00:00-05:00 -13.090746      83.526824      76.293122   \n","...                              ...            ...            ...   \n","2023-12-22 00:00:00-05:00 -10.098875      80.663261      83.908053   \n","2023-12-26 00:00:00-05:00  -4.185293      90.737754      85.128169   \n","2023-12-27 00:00:00-05:00  -1.730508      94.661774      88.687597   \n","2023-12-28 00:00:00-05:00  -4.538277      96.515307      93.971612   \n","2023-12-29 00:00:00-05:00 -11.740166      93.997016      95.058033   \n","\n","                           FISHERT_9_1  FISHERTs_9_1    ATRr_14           OBV  \\\n","Date                                                                            \n","1989-01-03 00:00:00-05:00     0.388937      0.797978   2.233492  2.680319e+10   \n","1989-01-04 00:00:00-05:00     0.248036      0.388937   2.391100  2.695289e+10   \n","1989-01-05 00:00:00-05:00     0.507648      0.248036   2.368880  2.712693e+10   \n","1989-01-06 00:00:00-05:00     0.908969      0.507648   2.346102  2.728826e+10   \n","1989-01-09 00:00:00-05:00     1.352505      0.908969   2.290666  2.745144e+10   \n","...                                ...           ...        ...           ...   \n","2023-12-22 00:00:00-05:00     2.832599      2.986470  41.184150  1.379242e+12   \n","2023-12-26 00:00:00-05:00     2.967104      2.832599  40.391734  1.381756e+12   \n","2023-12-27 00:00:00-05:00     3.241961      2.967104  38.684484  1.384504e+12   \n","2023-12-28 00:00:00-05:00     3.584504      3.241961  36.801294  1.387203e+12   \n","2023-12-29 00:00:00-05:00     2.846536      3.584504  36.775483  1.384077e+12   \n","\n","                              ZS_30   ENTP_10  \n","Date                                           \n","1989-01-03 00:00:00-05:00  0.229301  3.325722  \n","1989-01-04 00:00:00-05:00  1.197195  3.325502  \n","1989-01-05 00:00:00-05:00  1.312146  3.326466  \n","1989-01-06 00:00:00-05:00  1.453896  3.327660  \n","1989-01-09 00:00:00-05:00  1.477710  3.329150  \n","...                             ...       ...  \n","2023-12-22 00:00:00-05:00  1.638294  3.350524  \n","2023-12-26 00:00:00-05:00  1.736890  3.350531  \n","2023-12-27 00:00:00-05:00  1.720615  3.349759  \n","2023-12-28 00:00:00-05:00  1.610700  3.346839  \n","2023-12-29 00:00:00-05:00  1.372690  3.343309  \n","\n","[8817 rows x 32 columns]"],"text/html":["\n","  <div id=\"df-9e596bcc-33ef-4bc7-925c-0d2136fe4af7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Open</th>\n","      <th>High</th>\n","      <th>Low</th>\n","      <th>Close</th>\n","      <th>SMA_50</th>\n","      <th>SMA_200</th>\n","      <th>RSI_14</th>\n","      <th>BBL_5_2.0</th>\n","      <th>BBM_5_2.0</th>\n","      <th>BBU_5_2.0</th>\n","      <th>...</th>\n","      <th>SMIo_5_20_5</th>\n","      <th>WILLR_14</th>\n","      <th>STOCHk_14_3_3</th>\n","      <th>STOCHd_14_3_3</th>\n","      <th>FISHERT_9_1</th>\n","      <th>FISHERTs_9_1</th>\n","      <th>ATRr_14</th>\n","      <th>OBV</th>\n","      <th>ZS_30</th>\n","      <th>ENTP_10</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1989-01-03 00:00:00-05:00</th>\n","      <td>277.720001</td>\n","      <td>277.720001</td>\n","      <td>273.809998</td>\n","      <td>275.309998</td>\n","      <td>274.762200</td>\n","      <td>268.111900</td>\n","      <td>48.838065</td>\n","      <td>274.613084</td>\n","      <td>277.267993</td>\n","      <td>279.922903</td>\n","      <td>...</td>\n","      <td>-0.057734</td>\n","      <td>-77.409688</td>\n","      <td>54.631409</td>\n","      <td>58.669202</td>\n","      <td>0.388937</td>\n","      <td>0.797978</td>\n","      <td>2.233492</td>\n","      <td>2.680319e+10</td>\n","      <td>0.229301</td>\n","      <td>3.325722</td>\n","    </tr>\n","    <tr>\n","      <th>1989-01-04 00:00:00-05:00</th>\n","      <td>275.309998</td>\n","      <td>279.750000</td>\n","      <td>275.309998</td>\n","      <td>279.429993</td>\n","      <td>274.677599</td>\n","      <td>268.165350</td>\n","      <td>58.288271</td>\n","      <td>274.697225</td>\n","      <td>277.787994</td>\n","      <td>280.878764</td>\n","      <td>...</td>\n","      <td>-0.011009</td>\n","      <td>-15.361706</td>\n","      <td>54.945716</td>\n","      <td>57.522858</td>\n","      <td>0.248036</td>\n","      <td>0.388937</td>\n","      <td>2.391100</td>\n","      <td>2.695289e+10</td>\n","      <td>1.197195</td>\n","      <td>3.325502</td>\n","    </tr>\n","    <tr>\n","      <th>1989-01-05 00:00:00-05:00</th>\n","      <td>279.429993</td>\n","      <td>281.510010</td>\n","      <td>279.429993</td>\n","      <td>280.010010</td>\n","      <td>274.632200</td>\n","      <td>268.221200</td>\n","      <td>59.424554</td>\n","      <td>274.949366</td>\n","      <td>278.373999</td>\n","      <td>281.798632</td>\n","      <td>...</td>\n","      <td>0.014795</td>\n","      <td>-19.480489</td>\n","      <td>62.582706</td>\n","      <td>57.386610</td>\n","      <td>0.507648</td>\n","      <td>0.248036</td>\n","      <td>2.368880</td>\n","      <td>2.712693e+10</td>\n","      <td>1.312146</td>\n","      <td>3.326466</td>\n","    </tr>\n","    <tr>\n","      <th>1989-01-06 00:00:00-05:00</th>\n","      <td>280.010010</td>\n","      <td>282.059998</td>\n","      <td>280.010010</td>\n","      <td>280.670013</td>\n","      <td>274.598000</td>\n","      <td>268.280000</td>\n","      <td>60.735306</td>\n","      <td>274.775049</td>\n","      <td>278.628003</td>\n","      <td>282.480957</td>\n","      <td>...</td>\n","      <td>0.030888</td>\n","      <td>-16.848292</td>\n","      <td>82.769838</td>\n","      <td>66.766086</td>\n","      <td>0.908969</td>\n","      <td>0.507648</td>\n","      <td>2.346102</td>\n","      <td>2.728826e+10</td>\n","      <td>1.453896</td>\n","      <td>3.327660</td>\n","    </tr>\n","    <tr>\n","      <th>1989-01-09 00:00:00-05:00</th>\n","      <td>280.670013</td>\n","      <td>281.890015</td>\n","      <td>280.320007</td>\n","      <td>280.980011</td>\n","      <td>274.590000</td>\n","      <td>268.368150</td>\n","      <td>61.366579</td>\n","      <td>275.167730</td>\n","      <td>279.280005</td>\n","      <td>283.392280</td>\n","      <td>...</td>\n","      <td>0.037330</td>\n","      <td>-13.090746</td>\n","      <td>83.526824</td>\n","      <td>76.293122</td>\n","      <td>1.352505</td>\n","      <td>0.908969</td>\n","      <td>2.290666</td>\n","      <td>2.745144e+10</td>\n","      <td>1.477710</td>\n","      <td>3.329150</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2023-12-22 00:00:00-05:00</th>\n","      <td>4753.919922</td>\n","      <td>4772.939941</td>\n","      <td>4736.770020</td>\n","      <td>4754.629883</td>\n","      <td>4468.218584</td>\n","      <td>4335.861046</td>\n","      <td>71.069402</td>\n","      <td>4694.534676</td>\n","      <td>4741.732031</td>\n","      <td>4788.929386</td>\n","      <td>...</td>\n","      <td>-0.054860</td>\n","      <td>-10.098875</td>\n","      <td>80.663261</td>\n","      <td>83.908053</td>\n","      <td>2.832599</td>\n","      <td>2.986470</td>\n","      <td>41.184150</td>\n","      <td>1.379242e+12</td>\n","      <td>1.638294</td>\n","      <td>3.350524</td>\n","    </tr>\n","    <tr>\n","      <th>2023-12-26 00:00:00-05:00</th>\n","      <td>4758.859863</td>\n","      <td>4784.720215</td>\n","      <td>4758.450195</td>\n","      <td>4774.750000</td>\n","      <td>4477.157988</td>\n","      <td>4340.426846</td>\n","      <td>72.704374</td>\n","      <td>4694.610700</td>\n","      <td>4748.570020</td>\n","      <td>4802.529339</td>\n","      <td>...</td>\n","      <td>-0.035086</td>\n","      <td>-4.185293</td>\n","      <td>90.737754</td>\n","      <td>85.128169</td>\n","      <td>2.967104</td>\n","      <td>2.832599</td>\n","      <td>40.391734</td>\n","      <td>1.381756e+12</td>\n","      <td>1.736890</td>\n","      <td>3.350531</td>\n","    </tr>\n","    <tr>\n","      <th>2023-12-27 00:00:00-05:00</th>\n","      <td>4773.450195</td>\n","      <td>4785.390137</td>\n","      <td>4768.899902</td>\n","      <td>4781.580078</td>\n","      <td>4485.316992</td>\n","      <td>4345.055946</td>\n","      <td>73.256889</td>\n","      <td>4692.545294</td>\n","      <td>4751.212012</td>\n","      <td>4809.878729</td>\n","      <td>...</td>\n","      <td>-0.019366</td>\n","      <td>-1.730508</td>\n","      <td>94.661774</td>\n","      <td>88.687597</td>\n","      <td>3.241961</td>\n","      <td>2.967104</td>\n","      <td>38.684484</td>\n","      <td>1.384504e+12</td>\n","      <td>1.720615</td>\n","      <td>3.349759</td>\n","    </tr>\n","    <tr>\n","      <th>2023-12-28 00:00:00-05:00</th>\n","      <td>4786.439941</td>\n","      <td>4793.299805</td>\n","      <td>4780.979980</td>\n","      <td>4783.350098</td>\n","      <td>4493.519990</td>\n","      <td>4349.376246</td>\n","      <td>73.407118</td>\n","      <td>4738.605198</td>\n","      <td>4768.212012</td>\n","      <td>4797.818826</td>\n","      <td>...</td>\n","      <td>-0.009495</td>\n","      <td>-4.538277</td>\n","      <td>96.515307</td>\n","      <td>93.971612</td>\n","      <td>3.584504</td>\n","      <td>3.241961</td>\n","      <td>36.801294</td>\n","      <td>1.387203e+12</td>\n","      <td>1.610700</td>\n","      <td>3.346839</td>\n","    </tr>\n","    <tr>\n","      <th>2023-12-29 00:00:00-05:00</th>\n","      <td>4782.879883</td>\n","      <td>4788.430176</td>\n","      <td>4751.990234</td>\n","      <td>4769.830078</td>\n","      <td>4502.624590</td>\n","      <td>4353.765747</td>\n","      <td>70.164874</td>\n","      <td>4752.214119</td>\n","      <td>4772.828027</td>\n","      <td>4793.441935</td>\n","      <td>...</td>\n","      <td>-0.022409</td>\n","      <td>-11.740166</td>\n","      <td>93.997016</td>\n","      <td>95.058033</td>\n","      <td>2.846536</td>\n","      <td>3.584504</td>\n","      <td>36.775483</td>\n","      <td>1.384077e+12</td>\n","      <td>1.372690</td>\n","      <td>3.343309</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8817 rows × 32 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e596bcc-33ef-4bc7-925c-0d2136fe4af7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9e596bcc-33ef-4bc7-925c-0d2136fe4af7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9e596bcc-33ef-4bc7-925c-0d2136fe4af7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2a650512-f060-4e40-950a-495b11c44f8e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a650512-f060-4e40-950a-495b11c44f8e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2a650512-f060-4e40-950a-495b11c44f8e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_b1ebddfb-bc8d-488a-bc6d-7233769b66bd\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b1ebddfb-bc8d-488a-bc6d-7233769b66bd button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":5}],"source":["def extract_data(start_year, end_year=2023, ticker=\"^SPX\"):\n","    data = yf.Ticker(ticker).history(period=\"max\")\n","    data = data.dropna()\n","\n","    def add_features(data):\n","      # Assuming your DataFrame is named 'data'\n","      data.ta.sma(close=\"Close\", length=50, append=True)\n","      data.ta.sma(close=\"Close\", length=200, append=True)\n","      #data.ta.ichimoku(close=\"Close\", append=True)\n","      #data.ta.macd(close=\"Close\", append=True)\n","      data.ta.rsi(close=\"Close\", append=True)\n","      data.ta.bbands(close=\"Close\", append=True)\n","      data.ta.macd(close=\"Close\", append=True)\n","      data.ta.ichimoku(close=\"Close\", append=True)\n","      data.ta.smi(close=\"Close\", append=True)\n","      data.ta.willr(close=\"Close\", low=\"Low\", high=\"High\", append=True)\n","      data.ta.stoch(close=\"Close\", low=\"Low\", high=\"High\", append=True)\n","      data.ta.fisher(low=\"Low\", high=\"High\", append=True)\n","      data.ta.atr(low=\"Low\", high=\"High\", close=\"Close\", append=True)\n","      #data.ta.cdl_pattern(name=['eveningstar', '3whitesoldiers', 'morningstar', '3blackcrows', '3linestrike'])\n","      data.ta.obv(volume=\"Volume\", close=\"Close\", append=True)\n","      data.ta.zscore(close=\"Close\", append=True)\n","      data.ta.entropy(close=\"Close\", append=True)\n","      return data\n","\n","    data = add_features(data)\n","    ## Columns to Drop\n","    drop = ['Volume', 'Dividends', 'Stock Splits']\n","    data = data.drop(drop, axis=1)\n","    data = data.dropna()\n","    start_year = data.index[0].year if start_year is None else start_year\n","    data = data[data.index.year >= start_year-1] if start_year is not None else data\n","    data = data[data.index.year <= end_year] if end_year is not None else data\n","    #print(f\"start year: {start_year}\")\n","    #print(data)\n","    return data\n","\n","df = extract_data(1990)\n","#df = df['Close']\n","df"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1724913643402,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"KvAyssNdp7zD"},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","\n","class StockDataset(Dataset):\n","    def __init__(self, tickers, flag='train', size=None,\n","                 features='S', target='Close', scale=True, timeenc=0, freq='d', batch_size=5,\n","                 data_start_year=1990, data_end_year=2023):\n","        if size is None:\n","            self.seq_len = 24 * 4 * 4\n","            self.label_len = 24 * 4\n","            self.pred_len = 24 * 4\n","        else:\n","            self.seq_len = size[0]\n","            self.label_len = size[1]\n","            self.pred_len = size[2]\n","\n","        assert flag in ['train', 'test', 'val']\n","        type_map = {'train': 0, 'val': 1, 'test': 2}\n","        self.set_type = type_map[flag]\n","        self.data_start_year = data_start_year\n","        self.data_end_year = data_end_year\n","\n","        self.features = features\n","        self.target = target\n","        self.scale = scale\n","        self.timeenc = timeenc\n","        self.freq = freq\n","        self.scaler = StandardScaler()\n","        self.tickers = tickers.split()\n","        self.ticker_database = {}\n","        self.batch_size = batch_size\n","        self.min_year = 0\n","        self.__read_data__()\n","\n","    def __len__(self):\n","        return len(self.years) - 1\n","\n","    def get_min_year(self):\n","        min_year = 0\n","        for ticker in self.tickers:\n","            self.ticker_database[ticker] = extract_data(start_year=None, ticker=ticker)\n","            min_year = max(min_year, self.ticker_database[ticker].index.year.min())\n","        return min_year + 2\n","\n","    def __read_data__(self):\n","        print(f\"Loading following tickers: {self.tickers}\\n\")\n","\n","        self.min_year = max(self.get_min_year(), self.data_start_year)\n","        print(f\"Dataset Start Year: {self.min_year} | End Year: {self.data_end_year}\")\n","        for ticker in self.tickers:\n","            self.ticker_database[ticker] = extract_data(start_year=self.min_year, end_year=self.data_end_year, ticker=ticker)\n","            self.ticker_database[ticker]['year'] = self.ticker_database[ticker].index.year\n","\n","        self.years = self.ticker_database[self.tickers[0]]['year'].unique()\n","        print(f\"years: {self.years}\")\n","        self.data_by_year = {year: {ticker: self.ticker_database[ticker][self.ticker_database[ticker]['year'] == year] for ticker in self.tickers} for year in self.years}\n","\n","        self.data_len = len(self.years)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Final item form is a list containing [seq_x, seq_y, seq_x_mark, seq_y_mark, seq_x_dates, seq_y_dates]\n","        batches_x = batches[0]\n","        batches_y = batches[1]\n","        batches_x_mark = batches[2]\n","        batches_y_mark = batches[3]\n","        batches_x_dates = batches[4]\n","        batches_y_dates = batches[5]\n","\n","        Each batches, contain batch data of size (batch_size, seq_len, num_features)\n","        For example, if seq_len = 24, batch_size = 5, num_features = 32,\n","        Each item of batches_x is a tensor of (5, 24, 32)\n","        \"\"\"\n","        year = self.min_year + idx if idx >= 0 else self.max_year + idx + 1\n","        #print(f\"Stock Dataset Yeaer: {year}\")\n","        raw_datas = []\n","\n","        for ticker in self.tickers:\n","            if year - 1 in self.data_by_year:\n","                prev_year_data = self.data_by_year[year - 1][ticker].tail(self.seq_len + self.pred_len - 1)\n","                #print(f\"Prev year data for {ticker} in {year}: {prev_year_data.index}\")\n","            else:\n","                print(f\"Previous Year Data is Insufficient, Year: {year-1}, Ticker: {ticker}\")\n","                prev_year_data = pd.DataFrame()\n","\n","            ticker_data = pd.concat([prev_year_data, self.data_by_year[year][ticker]])\n","            #print(f\"Ticker data for {ticker} in {year}: {ticker_data.index}\")\n","            ticker_data['date'] = ticker_data.index\n","            raw_datas.append(ticker_data)\n","            \"\"\"print(f\"Ticker: {ticker}\")\n","            print(len(ticker_data))\n","            print(ticker_data)\"\"\"\n","\n","        seq_x = []\n","        seq_y = []\n","        seq_x_mark = []\n","        seq_y_mark = []\n","        seq_x_dates = []\n","        seq_y_dates = []\n","\n","        for item in raw_datas:\n","            x, y, x_mark, y_mark, x_dates, y_dates = self.make_data(item)\n","            seq_x.append(x)\n","            seq_y.append(y)\n","            seq_x_mark.append(x_mark)\n","            seq_y_mark.append(y_mark)\n","            seq_x_dates.append(x_dates)\n","            seq_y_dates.append(y_dates)\n","\n","        # Combine all tickers into a single batch and slice them into mini-batches\n","        #print(len(seq_x))\n","        return self.create_batches(seq_x, seq_y, seq_x_mark, seq_y_mark, seq_x_dates, seq_y_dates), year\n","\n","    def make_data(self, raw_data):\n","        cols = list(raw_data.columns)\n","        cols.remove(self.target)\n","        cols.remove('date')\n","        cols.remove('year')\n","        raw_data = raw_data[['date'] + cols + [self.target]]\n","\n","        if self.features == 'MS':\n","            cols_data = raw_data.columns[1:]\n","            data = raw_data[cols_data]\n","        elif self.features == 'S':\n","            data = raw_data['Close']\n","\n","        if self.scale:\n","            data = self.scaler.fit_transform(data.values)\n","\n","        data_stamp = time_features(pd.to_datetime(raw_data['date'].values), freq=self.freq)\n","        data_stamp = data_stamp.transpose(1, 0)\n","\n","        \"\"\"print(\"----Yearly data----\")\n","        print(data[0].shape)\n","        print(len(data))\n","        print(data)\"\"\"\n","\n","        seq_x = []\n","        seq_y = []\n","        seq_x_mark = []\n","        seq_y_mark = []\n","        x_dates = []\n","        y_dates = []\n","\n","        for i in range(len(data) - self.pred_len - self.seq_len + 1):\n","            s_begin = i\n","            s_end = s_begin + self.seq_len\n","            r_begin = s_end - self.label_len\n","            r_end = r_begin + self.label_len + self.pred_len\n","\n","            seq_x.append(data[s_begin:s_end])\n","            seq_y.append(data[r_begin:r_end])\n","            seq_x_mark.append(data_stamp[s_begin:s_end])\n","            seq_y_mark.append(data_stamp[r_begin:r_end])\n","            x_dates.append(raw_data['date'][s_begin:s_end])\n","            y_dates.append(raw_data['date'][r_begin:r_end])\n","            \"\"\"print(f\"data[s_begin:s_end]: {data[s_begin:s_end]}\")\n","            print(f\"data[r_begin:r_end]: {data[r_begin:r_end]}\")\n","            print(f\"raw_data['date'][s_begin:s_end]: {raw_data['date'][s_begin:s_end]}\")\n","            print(f\"raw_data['date'][r_begin:r_end]: {raw_data['date'][r_begin:r_end]}\")\n","            print(\"=======================================\")\"\"\"\n","\n","        return torch.tensor(np.array(seq_x)), torch.tensor(np.array(seq_y)), torch.tensor(np.array(seq_x_mark)), torch.tensor(np.array(seq_y_mark)), x_dates, y_dates\n","\n","    def create_batches(self, seq_x, seq_y, seq_x_mark, seq_y_mark, seq_x_dates, seq_y_dates, dates=None):\n","        batches_x = []\n","        batches_y = []\n","        batches_x_mark = []\n","        batches_y_mark = []\n","        batches_x_dates = []\n","        batches_y_dates = []\n","        batch_size = self.batch_size\n","        #print(dates[-1])\n","        for x, y, x_mark, y_mark, x_dates, y_dates in zip(seq_x, seq_y, seq_x_mark, seq_y_mark, seq_x_dates, seq_y_dates):\n","            #print(x.shape)\n","            for i in range(0, x.shape[0], batch_size):\n","                batches_x.append(x[i:i + batch_size])\n","                batches_y.append(y[i:i + batch_size])\n","                #print(f\"x[i:i + batch_size]: {x[i:i + batch_size].shape}\")\n","                #print(f\"y[i:i + batch_size]: {y[i:i + batch_size].shape}\")\n","                batches_x_mark.append(x_mark[i:i + batch_size])\n","                batches_y_mark.append(y_mark[i:i + batch_size])\n","                batches_x_dates.append(x_dates[i:i + batch_size])\n","                batches_y_dates.append(y_dates[i:i + batch_size])\n","                \"\"\"print(f\"x[i:i + batch_size]: {x[i:i + batch_size]}\")\n","                print(f\"y[i:i + batch_size]: {y[i:i + batch_size]}\")\n","                print(f\"x_dates[i:i + batch_size]: {x_dates[i:i + batch_size]}\")\n","                print(f\"y_dates[i:i + batch_size]: {y_dates[i:i + batch_size]}\")\n","                print(f\"------------------------------------------------------\")\"\"\"\n","\n","        return batches_x, batches_y, batches_x_mark, batches_y_mark, batches_x_dates, batches_y_dates\n","\n","    def inverse_transform(self, data):\n","        return self.scaler.inverse_transform(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":614,"status":"ok","timestamp":1724747779509,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"ugFs_doQF6EK","outputId":"06fbed18-94bb-4d9b-a4fd-58cb44a76f77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading following tickers: ['^SPX']\n","\n","Dataset Start Year: 1990 | End Year: 2023\n","years: [1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\n"," 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n"," 2017 2018 2019 2020 2021 2022 2023]\n"]}],"source":["dataset = StockDataset(tickers='^SPX',timeenc=1, freq='d', size=[36, 18, 1], features='MS')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XHX6kM4BZzx"},"outputs":[],"source":["batches, year = dataset[0]"]},{"cell_type":"markdown","metadata":{"id":"2W7rRMaXlW_P"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"2Ll-r4SeAg1x"},"source":["## CARD"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2654,"status":"ok","timestamp":1724913646031,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"BI0iDQy8liNO","outputId":"c484718d-2cbb-4f22-e9fb-d706a4c3c026"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"]}],"source":["!pip install einops"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1724913646031,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"FrSYbXQ7r65F"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":709,"status":"ok","timestamp":1724914542080,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"u_Si1jsNrfnY"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from einops import rearrange\n","import numpy as np\n","\n","class Transpose(nn.Module):\n","    def __init__(self, *dims, contiguous=False):\n","        super().__init__()\n","        self.dims, self.contiguous = dims, contiguous\n","    def forward(self, x):\n","        if self.contiguous: return x.transpose(*self.dims).contiguous()\n","        else: return x.transpose(*self.dims)\n","\n","\n","\n","\n","class Model(nn.Module):\n","    def __init__(self, config, **kwargs):\n","\n","        super().__init__()\n","        self.model = CARDformer(config)\n","        self.task_name = config.task_name\n","        self.time_linear = nn.Linear(configs.datetime_features, configs.enc_in)\n","    def forward(self, batch_x, x_mark_enc, *args, **kwargs):\n","        time_embeds = self.time_linear(x_mark_enc)\n","        x = batch_x * time_embeds\n","        x = x.permute(0,2,1)\n","\n","        mask = args[-1]\n","        x= self.model(x,mask = mask)\n","        if self.task_name != 'classification':\n","            x = x.permute(0,2,1)\n","        return x\n","\n","class CARDformer(nn.Module):\n","    def __init__(self,\n","                 config,**kwargs):\n","\n","        super().__init__()\n","\n","        self.patch_len  = config.patch_len\n","        self.stride = config.stride\n","        self.d_model = config.d_model\n","        self.task_name = config.task_name\n","        patch_num = int((config.seq_len - self.patch_len)/self.stride + 1)\n","        self.patch_num = patch_num\n","        self.W_pos_embed = nn.Parameter(torch.randn(patch_num,config.d_model)*1e-2)\n","        self.model_token_number = 0\n","\n","        if self.model_token_number > 0:\n","            self.model_token = nn.Parameter(torch.randn(config.enc_in,self.model_token_number,config.d_model)*1e-2)\n","\n","\n","        self.total_token_number = (self.patch_num  + self.model_token_number + 1)\n","        config.total_token_number = self.total_token_number\n","\n","        self.W_input_projection = nn.Linear(self.patch_len, config.d_model)\n","        self.input_dropout  = nn.Dropout(config.dropout)\n","\n","        self.use_statistic = config.use_statistic\n","        self.W_statistic = nn.Linear(2,config.d_model)\n","        self.cls = nn.Parameter(torch.randn(1,config.d_model)*1e-2)\n","\n","        if config.task_name == 'long_term_forecast' or config.task_name == 'short_term_forecast':\n","            self.W_out = nn.Linear((patch_num+1+self.model_token_number)*config.d_model, config.pred_len)\n","        elif config.task_name == 'imputation' or config.task_name == 'anomaly_detection':\n","            self.W_out = nn.Linear((patch_num+1+self.model_token_number)*config.d_model, config.seq_len)\n","        elif config.task_name == 'classification':\n","            self.W_out = nn.Linear(config.d_model*config.enc_in, config.num_class)\n","            self.dropout = nn.Dropout(config.dropout)\n","\n","        self.Attentions_over_token = nn.ModuleList([Attenion(config) for i in range(config.e_layers)])\n","        self.Attentions_over_channel = nn.ModuleList([Attenion(config,over_hidden = True) for i in range(config.e_layers)])\n","        self.Attentions_mlp = nn.ModuleList([nn.Linear(config.d_model,config.d_model)  for i in range(config.e_layers)])\n","        self.Attentions_dropout = nn.ModuleList([nn.Dropout(config.dropout)  for i in range(config.e_layers)])\n","        self.Attentions_norm = nn.ModuleList([nn.Sequential(Transpose(1,2), nn.BatchNorm1d(config.d_model,momentum = config.momentum), Transpose(1,2)) for i in range(config.e_layers)])\n","\n","    def forward(self, z,*args, **kwargs):\n","        b,c,s = z.shape\n","        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast' or self.task_name == 'anomaly_detection':\n","            z_mean = torch.mean(z,dim = (-1),keepdims = True)\n","            z_std = torch.std(z,dim = (-1),keepdims = True)\n","            z =  (z - z_mean)/(z_std + 1e-4)\n","\n","        elif self.task_name == 'imputation':\n","            mask = kwargs['mask'].permute(0,2,1)\n","            z_mean = torch.sum(z, dim=-1) / torch.sum(mask == 1, dim=-1)\n","            z_mean = z_mean.unsqueeze(-1)\n","            z = z - z_mean\n","            z = z.masked_fill(mask == 0, 0)\n","            z_std = torch.sqrt(torch.sum(z * z, dim=-1) /\n","                           torch.sum(mask == 1, dim=-1) + 1e-5)\n","            z_std = z_std.unsqueeze(-1)\n","            z /= z_std + 1e-4\n","\n","        zcube = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n","        #print(f\"zcube: {zcube.shape}\")\n","        z_embed = self.input_dropout(self.W_input_projection(zcube))\n","        #print(f\"z_embed: {z_embed.shape}\")\n","\n","        if self.use_statistic:\n","            z_stat = torch.cat((z_mean,z_std),dim = -1)\n","            if z_stat.shape[-2]>1:\n","                z_stat = (z_stat - torch.mean(z_stat,dim =-2,keepdims = True))/( torch.std(z_stat,dim =-2,keepdims = True)+1e-4)\n","            z_stat = self.W_statistic(z_stat)\n","            z_embed = torch.cat((z_stat.unsqueeze(-2),z_embed),dim = -2)\n","        else:\n","            cls_token = self.cls.repeat(z_embed.shape[0],z_embed.shape[1],1,1)\n","            z_embed = torch.cat((cls_token,z_embed),dim = -2)\n","\n","        inputs = z_embed\n","        b,c,t,h = inputs.shape\n","        for a_2,a_1,mlp,drop,norm  in zip(self.Attentions_over_token, self.Attentions_over_channel,self.Attentions_mlp ,self.Attentions_dropout,self.Attentions_norm ):\n","            output_1 = a_1(inputs.permute(0,2,1,3)).permute(0,2,1,3)\n","            output_2 = a_2(output_1)\n","            outputs = drop(mlp(output_1+output_2))+inputs\n","            outputs = norm(outputs.reshape(b*c,t,-1)).reshape(b,c,t,-1)\n","            inputs = outputs\n","\n","        if self.task_name != 'classification':\n","            z_out = self.W_out(outputs.reshape(b,c,-1))\n","            z = z_out *(z_std+1e-4)  + z_mean\n","        else:\n","            z = self.W_out(torch.mean(outputs[:,:,:,:],dim = -2).reshape(b,-1))\n","        return z\n","\n","\n","class Attenion(nn.Module):\n","    def __init__(self,config, over_hidden = False,trianable_smooth = False,untoken = False, *args, **kwargs):\n","        super().__init__()\n","\n","\n","        self.over_hidden = over_hidden\n","        self.untoken = untoken\n","        self.n_heads = config.n_heads\n","        self.c_in = config.enc_in\n","        self.qkv = nn.Linear(config.d_model, config.d_model * 3, bias=True)\n","\n","        self.attn_dropout = nn.Dropout(config.dropout)\n","        self.head_dim = config.d_model // config.n_heads\n","\n","        self.dropout_mlp = nn.Dropout(config.dropout)\n","        self.mlp = nn.Linear( config.d_model,  config.d_model)\n","\n","        self.norm_post1  = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(config.d_model,momentum = config.momentum), Transpose(1,2))\n","        self.norm_post2  = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(config.d_model,momentum = config.momentum), Transpose(1,2))\n","        self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(config.d_model,momentum = config.momentum), Transpose(1,2))\n","\n","        self.dp_rank = config.dp_rank\n","        self.dp_k = nn.Linear(self.head_dim, self.dp_rank)\n","        self.dp_v = nn.Linear(self.head_dim, self.dp_rank)\n","\n","\n","        self.ff_1 = nn.Sequential(nn.Linear(config.d_model, config.d_ff, bias=True),\n","                        nn.GELU(),\n","                        nn.Dropout(config.dropout),\n","                        nn.Linear(config.d_ff, config.d_model, bias=True)\n","                       )\n","        self.ff_2= nn.Sequential(nn.Linear(config.d_model, config.d_ff, bias=True),\n","                        nn.GELU(),\n","                        nn.Dropout(config.dropout),\n","                        nn.Linear(config.d_ff, config.d_model, bias=True)\n","                                )\n","        self.merge_size = config.merge_size\n","        ema_size = max(config.enc_in,config.total_token_number,config.dp_rank)\n","        ema_matrix = torch.zeros((ema_size,ema_size))\n","        alpha = config.alpha\n","        ema_matrix[0][0] = 1\n","        for i in range(1,config.total_token_number):\n","            for j in range(i):\n","                ema_matrix[i][j] =  ema_matrix[i-1][j]*(1-alpha)\n","            ema_matrix[i][i] = alpha\n","        self.register_buffer('ema_matrix',ema_matrix)\n","\n","    def ema(self,src):\n","        return torch.einsum('bnhad,ga ->bnhgd',src,self.ema_matrix[:src.shape[-2],:src.shape[-2]])\n","\n","    def ema_trianable(self,src):\n","        alpha = F.sigmoid(self.alpha)\n","        weights = alpha * (1 - alpha) ** self.arange[-src.shape[-2]:]\n","\n","        w_f = torch.fft.rfft(weights,n = src.shape[-2]*2)\n","        src_f = torch.fft.rfft(src.float(),dim = -2,n = src.shape[-2]*2)\n","        src_f = (src_f.permute(0,1,2,4,3)*w_f)\n","        src1 =torch.fft.irfft(src_f.float(),dim = -1,n=src.shape[-2]*2)[...,:src.shape[-2]].permute(0,1,2,4,3)#.half()\n","        return src1\n","\n","    def dynamic_projection(self,src,mlp):\n","        src_dp = mlp(src)\n","        src_dp = F.softmax(src_dp,dim = -1)\n","        src_dp = torch.einsum('bnhef,bnhec -> bnhcf',src,src_dp)\n","        return src_dp\n","\n","    def forward(self, src, *args,**kwargs):\n","        B,nvars, H, C, = src.shape\n","        qkv = self.qkv(src).reshape(B,nvars, H, 3, self.n_heads, C // self.n_heads).permute(3, 0, 1,4, 2, 5)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        if not self.over_hidden:\n","            attn_score_along_token = torch.einsum('bnhed,bnhfd->bnhef', self.ema(q), self.ema(k))/ self.head_dim ** -0.5\n","            attn_along_token = self.attn_dropout(F.softmax(attn_score_along_token, dim=-1) )\n","            output_along_token = torch.einsum('bnhef,bnhfd->bnhed', attn_along_token, v)\n","        else:\n","            v_dp,k_dp = self.dynamic_projection(v,self.dp_v) , self.dynamic_projection(k,self.dp_k)\n","            attn_score_along_token = torch.einsum('bnhed,bnhfd->bnhef', self.ema(q), self.ema(k_dp))/ self.head_dim ** -0.5\n","            attn_along_token = self.attn_dropout(F.softmax(attn_score_along_token, dim=-1) )\n","            output_along_token = torch.einsum('bnhef,bnhfd->bnhed', attn_along_token, v_dp)\n","\n","        attn_score_along_hidden = torch.einsum('bnhae,bnhaf->bnhef', q,k)/ q.shape[-2] ** -0.5\n","        attn_along_hidden = self.attn_dropout(F.softmax(attn_score_along_hidden, dim=-1) )\n","        output_along_hidden = torch.einsum('bnhef,bnhaf->bnhae', attn_along_hidden, v)\n","        merge_size = self.merge_size\n","        if not self.untoken:\n","            output1 = rearrange(output_along_token.reshape(B*nvars,-1,self.head_dim),\n","                            'bn (hl1 hl2 hl3) d -> bn  hl2 (hl3 hl1) d',\n","                            hl1 = self.n_heads//merge_size, hl2 = output_along_token.shape[-2] ,hl3 = merge_size\n","                            ).reshape(B*nvars,-1,self.head_dim*self.n_heads)\n","            output2 = rearrange(output_along_hidden.reshape(B*nvars,-1,self.head_dim),\n","                            'bn (hl1 hl2 hl3) d -> bn  hl2 (hl3 hl1) d',\n","                            hl1 = self.n_heads//merge_size, hl2 = output_along_token.shape[-2] ,hl3 = merge_size\n","                            ).reshape(B*nvars,-1,self.head_dim*self.n_heads)\n","\n","        output1 = self.norm_post1(output1)\n","        output1 = output1.reshape(B,nvars, -1, self.n_heads * self.head_dim)\n","        output2 = self.norm_post2(output2)\n","        output2 = output2.reshape(B,nvars, -1, self.n_heads * self.head_dim)\n","        src2 =  self.ff_1(output1)+self.ff_2(output2)\n","\n","        src = src + src2\n","        src = src.reshape(B*nvars, -1, self.n_heads * self.head_dim)\n","        src = self.norm_attn(src)\n","\n","        src = src.reshape(B,nvars, -1, self.n_heads * self.head_dim)\n","        return src"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"CfKQ7ePfg8mD","executionInfo":{"status":"ok","timestamp":1724914542080,"user_tz":-540,"elapsed":31,"user":{"displayName":"James Jeon","userId":"00410856990549462437"}}},"outputs":[],"source":["import torch\n","\n","def classification_evaluate_direction(output, batch_y):\n","    \"\"\"\n","    Evaluate the accuracy of direction predictions and compare it to a baseline model.\n","\n","    Parameters:\n","    output (torch.Tensor): Tensor of predicted values with shape (batch_size, 96, 1).\n","    batch_y (torch.Tensor): Tensor of true values with shape (batch_size, 96, 1).\n","\n","    Returns:\n","    correct_predictions (int): Number of times the predicted direction was correct.\n","    accuracy (float): Fraction of correct predictions out of total comparisons.\n","    baseline_correct (int): Number of times the baseline prediction was correct.\n","    baseline_accuracy (float): Fraction of correct predictions by the baseline out of total comparisons.\n","    \"\"\"\n","    # Compute the direction of change for each tensor\n","    def compute_direction(tensor):\n","        # Compute the difference between consecutive elements\n","        diff = tensor[:, 1:, :] - tensor[:, :-1, :]\n","        # Convert differences to binary direction indicators: 1 for increase, -1 for decrease\n","        direction = torch.sign(diff)\n","        return direction\n","\n","    # Get direction indicators for both tensors\n","    #print(batch_y[:,0,:].shape)\n","    output = torch.cat((batch_y[:, 0, :].unsqueeze(2), output), dim=1)\n","    #print(output)\n","    output_direction = compute_direction(output)\n","    batch_y_direction = compute_direction(batch_y)\n","    #print(f\"output_direction: {output_direction.shape} batch_y_direction: {batch_y_direction.shape}\")\n","    # Compare the directions\n","    correct_predictions = (output_direction == batch_y_direction).sum().item()\n","    #print(f\"correct_predictions: {correct_predictions}\")\n","    total_comparisons = output_direction.numel()\n","    #print(f\"total_comparisons: {total_comparisons}\")\n","    accuracy = correct_predictions / total_comparisons\n","\n","    # Compute baseline predictions (always predict increase)\n","    baseline_direction = torch.ones_like(output_direction)  # Baseline always predicts increase\n","    baseline_correct = (baseline_direction == batch_y_direction).sum().item()\n","    baseline_accuracy = baseline_correct / total_comparisons\n","\n","    return correct_predictions, baseline_correct, total_comparisons"]},{"cell_type":"code","source":["def compute_direction(tensor):\n","  batch_size = tensor.size(0)\n","  comparison = tensor[:, 1, 0] > tensor[:, 0, 0]\n","  # Convert the comparison result to a one-hot vector\n","  one_hot = torch.zeros(batch_size, 2)\n","  one_hot[comparison, 0] = 1  # [1, 0] when the last value is greater\n","  one_hot[~comparison, 1] = 1  # [0, 1] when the first value is greater\n","  return one_hot\n","\n","tensor = torch.randn(5, 2, 1)\n","print(tensor)\n","compute_direction(tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aj1gzvd95Oaq","executionInfo":{"status":"ok","timestamp":1724914543566,"user_tz":-540,"elapsed":523,"user":{"displayName":"James Jeon","userId":"00410856990549462437"}},"outputId":"5548b0fc-bffe-446b-a98d-17424b46a27b"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.7190],\n","         [-0.2459]],\n","\n","        [[ 0.9250],\n","         [ 0.8770]],\n","\n","        [[-0.1339],\n","         [ 0.8808]],\n","\n","        [[ 0.7759],\n","         [-0.1423]],\n","\n","        [[-0.9674],\n","         [-2.1729]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 1.],\n","        [0., 1.],\n","        [1., 0.],\n","        [0., 1.],\n","        [0., 1.]])"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"B3Q53Qu8lqvw"},"source":["# Prediction Test"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"qj1MAZX8spAo","executionInfo":{"status":"ok","timestamp":1724914543566,"user_tz":-540,"elapsed":2,"user":{"displayName":"James Jeon","userId":"00410856990549462437"}}},"outputs":[],"source":["from dataclasses import dataclass\n","@dataclass\n","class Args():\n","    freq: str = 'd'\n","    task_name: str = 'classification'\n","    num_class: int =2\n","    seq_len: int = 36\n","    label_len: int = 18\n","    pred_len: int = 1\n","    e_layers: int = 2\n","    d_layers: int = 1\n","    n_heads: int = 16\n","    top_k: int = 5\n","    factor: int = 1\n","    enc_in: int = 32\n","    dec_in: int = 32\n","    c_out: int = 1\n","    d_model: int = 128\n","    d_ff: int = 512\n","    patch_len: int = 16\n","    moving_avg: int = 25\n","    factor: int = 3\n","    distil: bool = True\n","    output_attention: bool = False\n","    patience: int = 400\n","    stride: int = 1\n","    learning_rate: float = 0.0005\n","    batch_size: int = 32\n","    embed: str = 'timeF'\n","    activation: str = 'gelu'\n","    dropout: float = 0.0\n","    loss: str = 'mse'\n","    data: str = 'custom'\n","    features: str = 'MS'\n","    train_epochs: int = 100\n","    use_statistic: bool = False\n","    mask_rate: float = 0.25\n","    anomaly_ratio: float = 0.25\n","    num_kernels: int = 6\n","    moving_avg: int = 25\n","    activation: str = 'gelu'\n","    fc_dropout: float = 0.3\n","    head_dropout: float = 0.3\n","    momentum: float = 0.1\n","    dp_rank: int = 8\n","    merge_size: int = 2\n","    alpha: float = 0.5\n","    beta: float = 0.5\n","\n","    ## Data\n","    batch_size: int = 32\n","    data_start_year: int = 1990\n","    data_end_year: int = 2023\n","    one_hot_datetime: bool = False\n","    datetime_features: int = 3\n","\n","    ## Training\n","    run_name: str = \"test\"\n","    validation_years: int = 1\n","    test_years: int = 1\n","    ticker: str = \"^SPX\"\n","    rolling_window: int = 10 # How many training years to be included in each training dataset\n","    window_epoch: int = 50 # How many epochs to train per dataset\n","    reset_model: bool = False\n","    save_folder: str = \"card_rolling\""]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":630,"status":"ok","timestamp":1724914545402,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"ZxpJvmBLtkDo","outputId":"ed86200d-d914-4ec6-c906-ea1e9134be79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading following tickers: ['^SPX']\n","\n","Dataset Start Year: 1990 | End Year: 2023\n","years: [1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\n"," 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n"," 2017 2018 2019 2020 2021 2022 2023]\n"]}],"source":["configs = Args()\n","card = Model(configs)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_optim = torch.optim.Adam(card.parameters(), lr=configs.learning_rate)\n","loss_fn = nn.BCEWithLogitsLoss()\n","dataset = StockDataset(tickers='^SPX',timeenc=1, freq='d', size=[36, 18, 1], features='MS')"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1646,"status":"ok","timestamp":1724914547045,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"0WdAM_e2qnVs","outputId":"9920dfbd-2846-4416-d33d-3cdfeecc2c05"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/22 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([5, 2]) | truth_direction: torch.Size([5, 2])\n","outputs: torch.Size([3, 2]) | truth_direction: torch.Size([3, 2])\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/22 [00:01<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Year: 1990 total_comparisons: 253 training_hit_ratio: 0.5059288537549407 | ol_hit_ratio: 0.5335968379446641\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","range_limit = configs.rolling_window+configs.validation_years+configs.test_years\n","loop_range = tqdm.tqdm(range(len(dataset)-range_limit))\n","card = card.to(device)\n","for iteration in loop_range:\n","  for window_iteration in range(configs.window_epoch):\n","    epoch_loss = []\n","    total_hits = 0\n","    total_data = 0\n","    total_ol = 0\n","    starting_idx = iteration // 10\n","    for i in range((configs.rolling_window+configs.validation_years+configs.test_years)):\n","        validation, test = False, False\n","        if i == configs.rolling_window:\n","          validation = True\n","        elif i == (configs.rolling_window+1):\n","          test = True\n","        else:\n","          batches, year  = dataset[i+iteration]\n","          batches_x, batches_y, batches_x_mark, batches_y_mark = batches[0], batches[1], batches[2], batches[3]\n","          for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(zip(batches_x, batches_y, batches_x_mark, batches_y_mark)):\n","            model_optim.zero_grad()\n","            batch_x = batch_x.float().to(device)\n","            batch_y = batch_y.float().to(device)\n","            true_batch_y = batch_y.float().to(device)\n","            batch_x_mark = batch_x_mark.float().to(device)\n","            batch_y_mark = batch_y_mark.float().to(device)\n","            dec_inp = torch.zeros_like(true_batch_y[:, -configs.pred_len:, :]).float()\n","            dec_inp = torch.cat([true_batch_y[:, :configs.label_len, :], dec_inp], dim=1).float().to(device)\n","            #print(batch_x.shape)\n","\n","            outputs = card(batch_x, batch_x_mark, dec_inp, None)\n","\n","\n","            f_dim = -1 if configs.features == 'MS' else 0\n","            batch_y_direction = batch_y[:, -(configs.pred_len+1):, f_dim:].to(device)\n","            batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(device)\n","            truth_direction = compute_direction(batch_y_direction).to(device)\n","            print(f\"outputs: {outputs.shape} | truth_direction: {truth_direction.shape}\")\n","            loss = loss_fn(outputs, truth_direction)\n","            epoch_loss.append(loss.item())\n","            loss.backward()\n","            model_optim.step()\n","\n","            p = outputs.detach().cpu().numpy().argmax(axis=1)\n","            l = truth_direction.detach().cpu().numpy().argmax(axis=1)\n","            only_long = np.zeros_like(l)\n","            correct_preds = np.sum(p == l)\n","            only_longs = np.sum(only_long == l)\n","\n","            #print(f\"outputs shape: {outputs.shape} | true_batch_y[:,-97:,:]: {true_batch_y[:,-97:,-1:].shape}\")\n","            total_hits += correct_preds\n","            total_data += outputs.size(0)\n","            total_ol += only_longs\n","        print(f\"Year: {year} total_comparisons: {total_data} training_hit_ratio: {total_hits/total_data} | ol_hit_ratio: {total_ol/total_data}\")\n","        break\n","    break\n","  break"]},{"cell_type":"markdown","metadata":{"id":"Ardiplb5lmA2"},"source":["# Trainer"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1724914643627,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"8k1dhSyP91Vy"},"outputs":[],"source":["class Trainer():\n","  def __init__(self, configs):\n","    self.configs = configs\n","\n","    ticker_str = configs.tickers.replace(\" \", \"_\")\n","    self.run_name = f\"[{ticker_str}]_valYrs:{self.configs.validation_years}_testYrs{self.configs.test_years}_reset:{self.configs.reset_model}_{configs.run_name}\"\n","    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    self.set_seed(configs.seed)\n","    self.dataset = self.make_data()\n","    self.model = Model(configs)\n","    self.model.to(self.device)\n","    self.loss_fn = nn.BCEWithLogitsLoss()\n","    self.model_optim = torch.optim.Adam(self.model.parameters(), lr=configs.learning_rate)\n","    self.writer = SummaryWriter(f\"/content/drive/MyDrive/code/fintransformer/runs/{self.run_name}\")\n","    path = f\"/content/drive/MyDrive/code/fintransformer/models/{self.configs.save_folder}/{self.run_name}\"\n","    if not os.path.exists(path):\n","      os.mkdir(path)\n","      print(f\"Save File Directory Made at {path}\\n\")\n","    else:\n","      print(f\"Directory Already Exists at {path}\")\n","\n","  def set_seed(self, seed):\n","      torch.manual_seed(seed)\n","      torch.cuda.manual_seed(seed)\n","      #torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n","      np.random.seed(seed)\n","      random.seed(seed)\n","      torch.backends.cudnn.deterministic = True\n","      torch.backends.cudnn.benchmark = False\n","\n","  def make_data(self):\n","    def collate_fn(batch):\n","      seq_x, seq_y, seq_x_mark, seq_y_mark, year = batch[0]\n","      return seq_x, seq_y, seq_x_mark, seq_y_mark, year\n","    dataset = StockDataset(tickers=self.configs.tickers, timeenc=1, freq='d', size=[self.configs.seq_len, self.configs.label_len, self.configs.pred_len],\n","                           features=self.configs.features, batch_size=self.configs.batch_size,\n","                           data_start_year=self.configs.data_start_year, data_end_year=self.configs.data_end_year)\n","    \"\"\"prices, labels, year = next(iter(val_loader))\n","    feature_num = prices.shape[2]\n","    label_num = labels.shape[1]\n","    print(f\"Data shape: {prices.shape} | Num Features: {feature_num}\")\"\"\"\n","    return dataset\n","\n","  def run(self):\n","    ### Training ###\n","    print(f\"Training model: {self.configs.run_name}\")\n","    print()\n","    range_limit = self.configs.rolling_window + self.configs.validation_years + self.configs.test_years\n","    loop_range = tqdm.tqdm(range(len(self.dataset)-range_limit))\n","    recent_save_path = \"\"\n","    val_acc_arr = []\n","    test_acc_arr = []\n","    for iteration in loop_range:\n","      highest_acc = 0.0\n","      highest_test_acc = 0.0\n","      if self.configs.reset_model and iteration != 0:\n","        self.model = Model(self.configs)\n","        self.model.to(self.device)\n","        self.model_optim = torch.optim.Adam(self.model.parameters(), lr=self.configs.learning_rate)\n","        print(f\"Model Has Been Reset to Random Parameters\")\n","      else:\n","        self.load_model(recent_save_path) if recent_save_path != \"\" else None\n","      for window_iteration in range(self.configs.window_epoch):\n","        train_loss, average_val_loss, average_val_accuracy, average_test_accuracy, validation_year, test_year = self.train(iteration, window_iteration, highest_acc)\n","        if average_val_accuracy > highest_acc:\n","            highest_acc = average_val_accuracy\n","            highest_test_acc = average_test_accuracy\n","            recent_save_path = self.save_model(average_val_accuracy, average_test_accuracy, window_iteration, validation_year, test_year)\n","      #self.writer.add_scalar(\"Train/Test Loss\", train_loss, iteration)\n","      val_acc_arr.append([highest_acc, validation_year])\n","      test_acc_arr.append([highest_test_acc, test_year])\n","      print(f\"***********************************************************\")\n","      print(f\"Highest Validation Accuracy in {validation_year[0]}~{validation_year[-1]}: {highest_acc*100:.2f}% | Highest Test Accuracy in {test_year}: {average_test_accuracy*100:.2f}%\")\n","      print(f\"***********************************************************\")\n","      print(\"=============================New Training Set====================================\")\n","    self.writer.flush()\n","    self.writer.close()\n","    #print(f\"Highest Accuracy in Validation Set: {highest_acc*100:.2f}\")\n","    print()\n","    for val, test in zip(val_acc_arr, test_acc_arr):\n","      print(f\"Validation Accuracy in {val[1][0]}~{val[1][-1]} : {val[0]*100:.2f}% | Test Accuracy in {test[1]}: {test[0]*100:.2f}%\")\n","\n","    average_first_elements_val = sum(item[0] for item in val_acc_arr) / len(val_acc_arr)\n","    average_first_elements_test = sum(item[0] for item in test_acc_arr) / len(test_acc_arr)\n","    print(f\"Average Validation Accuracy: {average_first_elements_val*100:.2f}% | Average Test Accuracy: {average_first_elements_test*100:.2f}%\")\n","    return\n","\n","  def train(self, iteration, window_iteration, highest_acc):\n","    epoch_loss = []\n","    val_epoch_loss = []\n","    training_start_year, training_end_year = 0, 0\n","    training_total_hits = 0\n","    training_total_data = 0\n","    training_total_ol = 0\n","    validation_year, test_year = [], 0\n","    validation_total_hits = 0\n","    validation_total_data = 0\n","    validation_total_ol = 0\n","    test_total_hits = 0\n","    test_total_data = 0\n","    test_total_ol = 0\n","    for i in range((self.configs.rolling_window+self.configs.validation_years+self.configs.test_years)):\n","      validation, test = False, False\n","      if i >= self.configs.rolling_window and i <= (self.configs.rolling_window+self.configs.validation_years-1):\n","        batches, year = self.dataset[i+iteration]\n","        #print(f\"validation year: {year}\")\n","        batches_x, batches_y, batches_x_mark, batches_y_mark = batches[0], batches[1], batches[2], batches[3]\n","        accuracy, only_long, val_loss_epoch, total_hits, total_data, total_ol = self.eval_once(batches_x, batches_y, batches_x_mark, batches_y_mark, year, highest_acc=0, train_acc=0)\n","        validation_total_hits += total_hits\n","        validation_total_data += total_data\n","        validation_total_ol += total_ol\n","        val_epoch_loss.append(val_loss_epoch)\n","        validation_year.append(year)\n","        validation = True\n","        print(f\"Year: {year} | Validation Accuracy: {(total_hits/total_data)*100:.2f}% | Only Long Accuracy: {(total_ol/total_data)*100:.2f}% | Highest Validation Accuracy: {highest_acc*100:.2f}%\")\n","      elif i >= (self.configs.rolling_window+self.configs.validation_years):\n","        batches, year = self.dataset[i+iteration]\n","        #print(f\"test year: {year}\")\n","        batches_x, batches_y, batches_x_mark, batches_y_mark = batches[0], batches[1], batches[2], batches[3]\n","        accuracy, only_long,  total_hits, total_data, total_ol = self.test(batches_x, batches_y, batches_x_mark, batches_y_mark, year)\n","        test_total_hits += total_hits\n","        test_total_data += total_data\n","        test_total_ol += total_ol\n","        test_year = year\n","        test = True\n","      else:\n","        self.model.train()\n","        batches, year = self.dataset[i+iteration]\n","        training_start_year = year if i == 0 else training_start_year\n","        training_end_year = year if i == self.configs.rolling_window-1 else training_end_year\n","        #print(f\"train year: {year}\")\n","        batches_x, batches_y, batches_x_mark, batches_y_mark = batches[0], batches[1], batches[2], batches[3]\n","        for j, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(zip(batches_x, batches_y, batches_x_mark, batches_y_mark)):\n","          self.model_optim.zero_grad()\n","\n","          batch_x = batch_x.float().to(self.device)\n","          batch_y = batch_y.float().to(self.device)\n","          true_batch_y = batch_y.float().to(self.device)\n","          batch_x_mark = batch_x_mark.float().to(self.device)\n","          batch_y_mark = batch_y_mark.float().to(self.device)\n","          #print(f\"batch_x: {batch_x.shape} batch_y: {batch_y.shape} batch_x_mark: {batch_x_mark.shape} batch_y_mark: {batch_y_mark.shape}\")\n","          dec_inp = torch.zeros_like(true_batch_y[:, -configs.pred_len:, :]).float()\n","          dec_inp = torch.cat([true_batch_y[:, :configs.label_len, :], dec_inp], dim=1).float().to(self.device)\n","          #print(batch_x.shape)\n","\n","          outputs = self.model(batch_x, batch_x_mark, dec_inp, None)\n","\n","          f_dim = -1 if configs.features == 'MS' else 0\n","          batch_y_direction = batch_y[:, -(configs.pred_len+1):, f_dim:].to(self.device)\n","          batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(self.device)\n","          truth_direction = compute_direction(batch_y_direction).to(self.device)\n","          #print(f\"outputs: {outputs.shape} | truth_direction: {truth_direction.shape}\")\n","          loss = self.loss_fn(outputs, truth_direction)\n","          epoch_loss.append(loss.item())\n","          loss.backward()\n","          self.model_optim.step()\n","\n","          p = outputs.detach().cpu().numpy().argmax(axis=1)\n","          l = truth_direction.detach().cpu().numpy().argmax(axis=1)\n","          only_long = np.zeros_like(l)\n","          correct_preds = np.sum(p == l)\n","          only_longs = np.sum(only_long == l)\n","          #print(f\"outputs: {outputs.shape} | batch_y: {batch_y.shape}\")\n","          #hit_count, only_long, count = calculate_hit(outputs, batch_y, configs.label_len-1)\n","\n","          training_total_hits += correct_preds\n","          training_total_data += outputs.size(0)\n","          training_total_ol += only_longs\n","\n","    loss_epoch = np.mean(epoch_loss)\n","    average_val_accuracy = validation_total_hits / validation_total_data\n","    average_val_only_long = validation_total_ol / validation_total_data\n","    average_val_loss = np.mean(val_epoch_loss)\n","    average_test_accuracy = test_total_hits / test_total_data\n","    average_test_only_long = test_total_ol / test_total_data\n","    train_accuracy = training_total_hits / training_total_data\n","    train_only_long = training_total_ol / training_total_data\n","    print(f\"Training Years: {training_start_year}~{training_end_year} | Training Accuracy: {train_accuracy*100:.2f}% | Training OL Accuracy: {train_only_long*100:.2f}%\")\n","    print(f\"Year: {validation_year[0]}~{validation_year[-1]} Validation Accuracy: {average_val_accuracy*100:.2f}% | Only Long Accuracy: {average_val_only_long*100:.2f}% | Highest Validation Accuracy: {highest_acc*100:.2f}%\")\n","    print(f\"Year: {test_year} | Test Accuracy: {average_test_accuracy*100:.2f}% | Only Long Accuracy: {average_test_only_long*100:.2f}%\")\n","    print(f\"------------------------------------------------\")\n","    self.writer.add_scalar(f\"Train:{training_start_year}~{training_end_year}/Train Accuracy\", train_accuracy, window_iteration)\n","    self.writer.add_scalar(f\"Train:{training_start_year}~{training_end_year}/Train Loss\", loss_epoch, window_iteration)\n","    self.writer.add_scalar(f\"Train:{training_start_year}~{training_end_year}/Validation Loss\", average_val_loss, window_iteration)\n","    self.writer.add_scalar(f\"Train:{training_start_year}~{training_end_year}/Validation Accuracy\", average_val_accuracy, window_iteration)\n","    self.writer.add_scalar(f\"Train:{training_start_year}~{training_end_year}/Test Accuracy\", average_test_accuracy, window_iteration)\n","      #print(f\"Year: {year} training_hit_ratio: {hit_count/batch_x.shape[0]} | ol_hit_ratio: {only_long/batch_x.shape[0]}\")\n","\n","    #print(f\"train accuracy: {accuracy*100:.2f}% | only_long: {only_long*100:.2f}%\")\n","    return loss_epoch, average_val_loss, average_val_accuracy, average_test_accuracy, validation_year, test_year\n","\n","  def eval_once(self, batches_x, batches_y, batches_x_mark, batches_y_mark, year, highest_acc, train_acc, last=False):\n","    self.model.eval()\n","    epoch_loss = []\n","    total_hits = 0\n","    total_data = 0\n","    total_ol = 0\n","    with torch.no_grad():\n","      for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(zip(batches_x, batches_y, batches_x_mark, batches_y_mark)):\n","        batch_x = batch_x.float().to(self.device)\n","        batch_y = batch_y.float().to(self.device)\n","        true_batch_y = batch_y.float().to(self.device)\n","        batch_x_mark = batch_x_mark.float().to(self.device)\n","        batch_y_mark = batch_y_mark.float().to(self.device)\n","        #print(f\"batch_x: {batch_x.shape} batch_y: {batch_y.shape} batch_x_mark: {batch_x_mark.shape} batch_y_mark: {batch_y_mark.shape}\")\n","        dec_inp = torch.zeros_like(true_batch_y[:, -configs.pred_len:, :]).float()\n","        dec_inp = torch.cat([true_batch_y[:, :configs.label_len, :], dec_inp], dim=1).float().to(self.device)\n","        #print(batch_x.shape)\n","\n","        outputs = self.model(batch_x, batch_x_mark, dec_inp, None)\n","\n","        f_dim = -1 if configs.features == 'MS' else 0\n","        batch_y_direction = batch_y[:, -(configs.pred_len+1):, f_dim:].to(self.device)\n","        batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(self.device)\n","        truth_direction = compute_direction(batch_y_direction).to(self.device)\n","\n","\n","        loss = self.loss_fn(outputs, truth_direction)\n","        epoch_loss.append(loss.item())\n","\n","        p = outputs.detach().cpu().numpy().argmax(axis=1)\n","        l = truth_direction.detach().cpu().numpy().argmax(axis=1)\n","        only_long = np.zeros_like(l)\n","        correct_preds = np.sum(p == l)\n","        only_longs = np.sum(only_long == l)\n","\n","        #hit_count, only_long, count = calculate_hit(outputs, batch_y, configs.label_len-1)\n","        total_hits += correct_preds\n","        total_data += outputs.size(0)\n","        total_ol += only_longs\n","        #print(f\"Year: {year} hit_ratio: {(hit_count/count)*100:.2f}% | ol_hit_ratio: {(only_long/count)*100:.2f}%\")\n","\n","    accuracy = total_hits / total_data\n","    long_strategy = total_ol / total_data\n","    loss_epoch = np.mean(epoch_loss)\n","    #print(f\"validation accuracy: {accuracy*100:.2f}% | val highest_acc: {highest_acc*100:.2f}% | val only long strategy: {long_strategy*100:.2f}%\")\n","    return accuracy, long_strategy, loss_epoch, total_hits, total_data, total_ol\n","\n","  def test(self, batches_x, batches_y, batches_x_mark, batches_y_mark, year):\n","    self.model.eval()\n","    total_hits = 0\n","    total_data = 0\n","    total_ol = 0\n","    with torch.no_grad():\n","      for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(zip(batches_x, batches_y, batches_x_mark, batches_y_mark)):\n","        batch_x = batch_x.float().to(self.device)\n","        batch_y = batch_y.float().to(self.device)\n","        true_batch_y = batch_y.float().to(self.device)\n","        batch_x_mark = batch_x_mark.float().to(self.device)\n","        batch_y_mark = batch_y_mark.float().to(self.device)\n","        #print(f\"batch_x: {batch_x.shape} batch_y: {batch_y.shape} batch_x_mark: {batch_x_mark.shape} batch_y_mark: {batch_y_mark.shape}\")\n","        dec_inp = torch.zeros_like(true_batch_y[:, -configs.pred_len:, :]).float()\n","        dec_inp = torch.cat([true_batch_y[:, :configs.label_len, :], dec_inp], dim=1).float().to(self.device)\n","        #print(batch_x.shape)\n","\n","        outputs = self.model(batch_x, batch_x_mark, dec_inp, None)\n","\n","        f_dim = -1 if configs.features == 'MS' else 0\n","        batch_y_direction = batch_y[:, -(configs.pred_len+1):, f_dim:].to(self.device)\n","        batch_y = batch_y[:, -configs.pred_len:, f_dim:].to(self.device)\n","        truth_direction = compute_direction(batch_y_direction).to(self.device)\n","\n","        p = outputs.detach().cpu().numpy().argmax(axis=1)\n","        l = truth_direction.detach().cpu().numpy().argmax(axis=1)\n","        only_long = np.zeros_like(l)\n","        correct_preds = np.sum(p == l)\n","        only_longs = np.sum(only_long == l)\n","\n","        #hit_count, only_long, count = calculate_hit(outputs, batch_y, configs.label_len-1)\n","        total_hits += correct_preds\n","        total_data += outputs.size(0)\n","        total_ol += only_longs\n","        #print(f\"Year: {year} hit_ratio: {(hit_count/count)*100:.2f}% | ol_hit_ratio: {(only_long/count)*100:.2f}%\")\n","\n","    accuracy = total_hits / total_data\n","    long_strategy = total_ol / total_data\n","    #print(f\"test accuracy: {accuracy*100:.2f}% | test only long strategy: {long_strategy*100:.2f}%\")\n","    return accuracy, long_strategy, total_hits, total_data, total_ol\n","\n","  def save_model(self, val_acc, test_acc, epoch, validation_year, test_year):\n","    PATH = f\"/content/drive/MyDrive/code/fintransformer/models/{self.configs.save_folder}/{self.run_name}/reset:{self.configs.reset_model}_valYear:{validation_year}_testYear:{test_year}.pt\"\n","    torch.save({\n","            'model_state_dict': self.model.state_dict(),\n","            'model_optimizer_state_dict': self.model_optim.state_dict(),\n","            'val_acc': val_acc,\n","            'validation_year': validation_year,\n","            'test_acc': test_acc,\n","            'test_year': test_year,\n","            'configs': self.configs}, PATH)\n","    print(f\"Model Saved at {PATH}\")\n","    return PATH\n","\n","  def load_model(self, path):\n","    checkpoint = torch.load(path, map_location=torch.device(self.device))\n","    self.model.load_state_dict(checkpoint['model_state_dict'])\n","    self.model_optim.load_state_dict(checkpoint['model_optimizer_state_dict'])\n","    print(f\"Model loaded from {path}\")\n","    print(f\"Model Validation Accuracy: {checkpoint['val_acc']*100:.2f}% | Test Accuracy: {checkpoint['test_acc']*100:.2f}%\")\n","    try:\n","      self.configs = checkpoint['configs']\n","    except:\n","      print(\"No Configs Found in torch file\")\n","    return"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724914644323,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"27fdwI7HCCqr"},"outputs":[],"source":["from dataclasses import dataclass\n","@dataclass\n","class Args():\n","    freq: str = 'd'\n","    task_name: str = 'classification'\n","    num_class: int = 2\n","    seq_len: int = 36\n","    label_len: int = 18\n","    pred_len: int = 1\n","    e_layers: int = 2\n","    d_layers: int = 1\n","    n_heads: int = 16\n","    top_k: int = 5\n","    factor: int = 1\n","    enc_in: int = 32\n","    dec_in: int = 32\n","    c_out: int = 1\n","    d_model: int = 128\n","    d_ff: int = 512\n","    patch_len: int = 16\n","    moving_avg: int = 25\n","    factor: int = 3\n","    distil: bool = True\n","    output_attention: bool = False\n","    patience: int = 400\n","    stride: int = 1\n","    learning_rate: float = 0.0005\n","    batch_size: int = 32\n","    embed: str = 'timeF'\n","    activation: str = 'gelu'\n","    dropout: float = 0.0\n","    loss: str = 'mse'\n","    data: str = 'custom'\n","    features: str = 'MS'\n","    train_epochs: int = 100\n","    use_statistic: bool = False\n","    mask_rate: float = 0.25\n","    anomaly_ratio: float = 0.25\n","    num_kernels: int = 6\n","    moving_avg: int = 25\n","    activation: str = 'gelu'\n","    fc_dropout: float = 0.3\n","    head_dropout: float = 0.3\n","    momentum: float = 0.1\n","    dp_rank: int = 8\n","    merge_size: int = 2\n","    alpha: float = 0.5\n","    beta: float = 0.5\n","\n","    ## Data\n","    batch_size: int = 32\n","    data_start_year: int = 1990\n","    data_end_year: int = 2023\n","    one_hot_datetime: bool = False\n","    datetime_features: int = 3 ## Change to 55 if one_hot_datetime = True\n","\n","    ## Training\n","    run_name: str = \"scale_run2\"\n","    seed: int = 2024\n","    validation_years: int = 2\n","    test_years: int = 1\n","    tickers: str = \"^SPX\" #\"goog amzn wmt xom brk-a lly ge lin pld aapl nee\"\n","    rolling_window: int = 10 # How many training years to be included in each training dataset\n","    window_epoch: int = 50 # How many epochs to train per dataset\n","    reset_model: bool = False\n","    save_folder: str = \"card_classification\""]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2349,"status":"ok","timestamp":1724914646669,"user":{"displayName":"James Jeon","userId":"00410856990549462437"},"user_tz":-540},"id":"UoaFH-QICjrR","outputId":"a19323ed-7d93-4428-a881-025d93101f04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading following tickers: ['^SPX']\n","\n","Dataset Start Year: 1990 | End Year: 2023\n","years: [1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002\n"," 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n"," 2017 2018 2019 2020 2021 2022 2023]\n","Directory Already Exists at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2\n"]}],"source":["configs = Args()\n","card_trainer = Trainer(configs)"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"R0V5js54cuCT","executionInfo":{"status":"error","timestamp":1724914881105,"user_tz":-540,"elapsed":234456,"user":{"displayName":"James Jeon","userId":"00410856990549462437"}},"outputId":"bf1aa4da-3c4b-49e1-8fe6-35e7fae1950e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training model: scale_run2\n","\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/21 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Year: 2000 | Validation Accuracy: 53.17% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 0.00%\n","Year: 2001 | Validation Accuracy: 52.82% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 0.00%\n","Training Years: 1990~1999 | Training Accuracy: 52.02% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 53.00% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 0.00%\n","Year: 2002 | Test Accuracy: 50.00% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 53.57% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 53.00%\n","Year: 2001 | Validation Accuracy: 51.21% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 53.00%\n","Training Years: 1990~1999 | Training Accuracy: 54.55% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 52.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 53.00%\n","Year: 2002 | Test Accuracy: 50.40% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 54.37% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 53.00%\n","Year: 2001 | Validation Accuracy: 51.21% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 53.00%\n","Training Years: 1990~1999 | Training Accuracy: 57.04% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 52.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 53.00%\n","Year: 2002 | Test Accuracy: 49.60% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 55.16% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 53.00%\n","Year: 2001 | Validation Accuracy: 51.61% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 53.00%\n","Training Years: 1990~1999 | Training Accuracy: 57.40% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 53.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 53.00%\n","Year: 2002 | Test Accuracy: 49.21% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 55.16% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 53.40%\n","Year: 2001 | Validation Accuracy: 51.21% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 53.40%\n","Training Years: 1990~1999 | Training Accuracy: 57.63% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 53.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 53.40%\n","Year: 2002 | Test Accuracy: 50.00% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 53.57% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 53.40%\n","Year: 2001 | Validation Accuracy: 54.44% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 53.40%\n","Training Years: 1990~1999 | Training Accuracy: 57.28% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 54.00% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 53.40%\n","Year: 2002 | Test Accuracy: 50.79% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 53.97% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 54.00%\n","Year: 2001 | Validation Accuracy: 55.24% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 54.00%\n","Training Years: 1990~1999 | Training Accuracy: 57.67% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 54.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 54.00%\n","Year: 2002 | Test Accuracy: 50.00% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 55.56% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 54.60%\n","Year: 2001 | Validation Accuracy: 56.05% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 54.60%\n","Training Years: 1990~1999 | Training Accuracy: 58.98% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 55.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 54.60%\n","Year: 2002 | Test Accuracy: 51.98% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 53.97% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 55.65% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 59.26% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 54.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 51.19% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 55.95% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 55.24% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 58.11% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 55.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 51.59% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 54.37% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 52.82% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 59.22% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 53.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 49.60% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 56.35% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 54.03% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 61.00% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 55.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 47.22% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 55.16% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 54.03% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 60.80% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 54.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 49.60% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 56.35% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 54.03% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 61.23% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 55.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 49.21% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 55.56% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 54.03% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 61.95% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 54.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 50.00% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 56.75% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 52.02% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 62.78% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 54.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 53.17% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 55.56% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 54.44% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 61.87% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 55.00% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 48.81% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 53.17% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 53.23% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 63.77% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 53.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 52.78% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 49.60% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 50.00% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 63.77% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 49.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 44.84% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 55.56% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 55.80%\n","Year: 2001 | Validation Accuracy: 56.85% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 55.80%\n","Training Years: 1990~1999 | Training Accuracy: 63.84% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 56.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 55.80%\n","Year: 2002 | Test Accuracy: 49.60% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 57.54% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 56.20%\n","Year: 2001 | Validation Accuracy: 55.24% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 56.20%\n","Training Years: 1990~1999 | Training Accuracy: 64.68% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 56.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 56.20%\n","Year: 2002 | Test Accuracy: 50.40% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 52.78% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 56.40%\n","Year: 2001 | Validation Accuracy: 52.42% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 56.40%\n","Training Years: 1990~1999 | Training Accuracy: 64.20% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 52.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 56.40%\n","Year: 2002 | Test Accuracy: 45.24% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 57.54% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 56.40%\n","Year: 2001 | Validation Accuracy: 58.06% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 56.40%\n","Training Years: 1990~1999 | Training Accuracy: 65.43% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 57.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 56.40%\n","Year: 2002 | Test Accuracy: 50.40% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 61.51% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 57.80%\n","Year: 2001 | Validation Accuracy: 56.85% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 57.80%\n","Training Years: 1990~1999 | Training Accuracy: 66.06% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 59.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 57.80%\n","Year: 2002 | Test Accuracy: 48.41% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 56.75% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 59.20%\n","Year: 2001 | Validation Accuracy: 58.87% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 59.20%\n","Training Years: 1990~1999 | Training Accuracy: 67.64% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 57.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 59.20%\n","Year: 2002 | Test Accuracy: 51.98% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 53.57% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 59.20%\n","Year: 2001 | Validation Accuracy: 51.21% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 59.20%\n","Training Years: 1990~1999 | Training Accuracy: 66.50% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 52.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 59.20%\n","Year: 2002 | Test Accuracy: 47.22% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 58.33% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 59.20%\n","Year: 2001 | Validation Accuracy: 58.47% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 59.20%\n","Training Years: 1990~1999 | Training Accuracy: 68.91% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 58.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 59.20%\n","Year: 2002 | Test Accuracy: 57.14% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 57.14% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 59.20%\n","Year: 2001 | Validation Accuracy: 58.47% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 59.20%\n","Training Years: 1990~1999 | Training Accuracy: 69.42% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 57.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 59.20%\n","Year: 2002 | Test Accuracy: 51.98% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 59.92% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 59.20%\n","Year: 2001 | Validation Accuracy: 63.31% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 59.20%\n","Training Years: 1990~1999 | Training Accuracy: 70.57% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 61.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 59.20%\n","Year: 2002 | Test Accuracy: 50.00% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 58.73% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 61.60%\n","Year: 2001 | Validation Accuracy: 61.69% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 61.60%\n","Training Years: 1990~1999 | Training Accuracy: 72.23% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 60.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 61.60%\n","Year: 2002 | Test Accuracy: 59.13% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 61.90% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 61.60%\n","Year: 2001 | Validation Accuracy: 60.89% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 61.60%\n","Training Years: 1990~1999 | Training Accuracy: 73.06% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 61.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 61.60%\n","Year: 2002 | Test Accuracy: 59.52% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 66.27% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 61.60%\n","Year: 2001 | Validation Accuracy: 61.29% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 61.60%\n","Training Years: 1990~1999 | Training Accuracy: 71.80% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 63.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 61.60%\n","Year: 2002 | Test Accuracy: 55.95% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 69.05% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 63.80%\n","Year: 2001 | Validation Accuracy: 63.71% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 63.80%\n","Training Years: 1990~1999 | Training Accuracy: 76.62% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 66.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 63.80%\n","Year: 2002 | Test Accuracy: 63.49% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 68.25% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 66.40%\n","Year: 2001 | Validation Accuracy: 68.55% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 66.40%\n","Training Years: 1990~1999 | Training Accuracy: 77.45% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 68.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 66.40%\n","Year: 2002 | Test Accuracy: 61.11% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 77.38% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 68.40%\n","Year: 2001 | Validation Accuracy: 74.60% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 68.40%\n","Training Years: 1990~1999 | Training Accuracy: 82.00% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 76.00% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 68.40%\n","Year: 2002 | Test Accuracy: 66.67% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 76.98% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 76.00%\n","Year: 2001 | Validation Accuracy: 73.39% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 76.00%\n","Training Years: 1990~1999 | Training Accuracy: 85.13% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 75.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 76.00%\n","Year: 2002 | Test Accuracy: 72.22% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 81.35% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 76.00%\n","Year: 2001 | Validation Accuracy: 77.02% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 76.00%\n","Training Years: 1990~1999 | Training Accuracy: 88.01% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 79.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 76.00%\n","Year: 2002 | Test Accuracy: 78.57% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 82.54% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 79.20%\n","Year: 2001 | Validation Accuracy: 80.65% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 79.20%\n","Training Years: 1990~1999 | Training Accuracy: 86.00% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 81.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 79.20%\n","Year: 2002 | Test Accuracy: 72.62% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 86.11% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 81.60%\n","Year: 2001 | Validation Accuracy: 83.06% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 81.60%\n","Training Years: 1990~1999 | Training Accuracy: 92.52% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 84.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 81.60%\n","Year: 2002 | Test Accuracy: 77.38% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 86.11% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 84.60%\n","Year: 2001 | Validation Accuracy: 83.87% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 84.60%\n","Training Years: 1990~1999 | Training Accuracy: 94.38% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 85.00% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 84.60%\n","Year: 2002 | Test Accuracy: 80.16% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 85.32% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 85.00%\n","Year: 2001 | Validation Accuracy: 83.87% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 85.00%\n","Training Years: 1990~1999 | Training Accuracy: 96.12% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 84.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 85.00%\n","Year: 2002 | Test Accuracy: 80.95% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 90.87% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 85.00%\n","Year: 2001 | Validation Accuracy: 88.71% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 85.00%\n","Training Years: 1990~1999 | Training Accuracy: 97.35% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 89.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 85.00%\n","Year: 2002 | Test Accuracy: 84.52% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 90.08% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 89.80%\n","Year: 2001 | Validation Accuracy: 86.69% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 89.80%\n","Training Years: 1990~1999 | Training Accuracy: 97.39% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 88.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 89.80%\n","Year: 2002 | Test Accuracy: 82.14% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 90.87% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 89.80%\n","Year: 2001 | Validation Accuracy: 88.71% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 89.80%\n","Training Years: 1990~1999 | Training Accuracy: 97.11% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 89.80% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 89.80%\n","Year: 2002 | Test Accuracy: 84.52% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 91.27% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 89.80%\n","Year: 2001 | Validation Accuracy: 89.92% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 89.80%\n","Training Years: 1990~1999 | Training Accuracy: 96.64% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 90.60% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 89.80%\n","Year: 2002 | Test Accuracy: 87.70% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 88.10% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 90.60%\n","Year: 2001 | Validation Accuracy: 88.71% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 90.60%\n","Training Years: 1990~1999 | Training Accuracy: 97.43% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 88.40% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 90.60%\n","Year: 2002 | Test Accuracy: 87.30% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 93.25% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 90.60%\n","Year: 2001 | Validation Accuracy: 89.11% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 90.60%\n","Training Years: 1990~1999 | Training Accuracy: 99.01% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 91.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 90.60%\n","Year: 2002 | Test Accuracy: 90.08% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 86.51% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 91.20%\n","Year: 2001 | Validation Accuracy: 85.48% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 91.20%\n","Training Years: 1990~1999 | Training Accuracy: 99.41% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 86.00% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 91.20%\n","Year: 2002 | Test Accuracy: 80.16% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Year: 2000 | Validation Accuracy: 93.25% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 91.20%\n","Year: 2001 | Validation Accuracy: 91.13% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 91.20%\n","Training Years: 1990~1999 | Training Accuracy: 98.97% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 92.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 91.20%\n","Year: 2002 | Test Accuracy: 91.27% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Year: 2000 | Validation Accuracy: 93.25% | Only Long Accuracy: 47.62% | Highest Validation Accuracy: 92.20%\n","Year: 2001 | Validation Accuracy: 91.13% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 92.20%\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 1/21 [03:21<1:07:12, 201.63s/it]"]},{"output_type":"stream","name":"stdout","text":["Training Years: 1990~1999 | Training Accuracy: 99.37% | Training OL Accuracy: 53.60%\n","Year: 2000~2001 Validation Accuracy: 92.20% | Only Long Accuracy: 47.80% | Highest Validation Accuracy: 92.20%\n","Year: 2002 | Test Accuracy: 90.08% | Only Long Accuracy: 44.44%\n","------------------------------------------------\n","***********************************************************\n","Highest Validation Accuracy in 2000~2001: 92.20% | Highest Test Accuracy in 2002: 90.08%\n","***********************************************************\n","=============================New Training Set====================================\n","Model loaded from /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2000, 2001]_testYear:2002.pt\n","Model Validation Accuracy: 92.20% | Test Accuracy: 91.27%\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-53-b4024b488a24>:294: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(path, map_location=torch.device(self.device))\n"]},{"output_type":"stream","name":"stdout","text":["Year: 2001 | Validation Accuracy: 85.48% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 0.00%\n","Year: 2002 | Validation Accuracy: 88.49% | Only Long Accuracy: 44.44% | Highest Validation Accuracy: 0.00%\n","Training Years: 1991~2000 | Training Accuracy: 98.73% | Training OL Accuracy: 53.03%\n","Year: 2001~2002 Validation Accuracy: 87.00% | Only Long Accuracy: 46.20% | Highest Validation Accuracy: 0.00%\n","Year: 2003 | Test Accuracy: 82.94% | Only Long Accuracy: 54.37%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2001, 2002]_testYear:2003.pt\n","Year: 2001 | Validation Accuracy: 92.74% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 87.00%\n","Year: 2002 | Validation Accuracy: 91.27% | Only Long Accuracy: 44.44% | Highest Validation Accuracy: 87.00%\n","Training Years: 1991~2000 | Training Accuracy: 97.19% | Training OL Accuracy: 53.03%\n","Year: 2001~2002 Validation Accuracy: 92.00% | Only Long Accuracy: 46.20% | Highest Validation Accuracy: 87.00%\n","Year: 2003 | Test Accuracy: 84.92% | Only Long Accuracy: 54.37%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2001, 2002]_testYear:2003.pt\n","Year: 2001 | Validation Accuracy: 91.53% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 92.00%\n","Year: 2002 | Validation Accuracy: 90.48% | Only Long Accuracy: 44.44% | Highest Validation Accuracy: 92.00%\n","Training Years: 1991~2000 | Training Accuracy: 98.06% | Training OL Accuracy: 53.03%\n","Year: 2001~2002 Validation Accuracy: 91.00% | Only Long Accuracy: 46.20% | Highest Validation Accuracy: 92.00%\n","Year: 2003 | Test Accuracy: 89.29% | Only Long Accuracy: 54.37%\n","------------------------------------------------\n","Year: 2001 | Validation Accuracy: 91.94% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 92.00%\n","Year: 2002 | Validation Accuracy: 93.65% | Only Long Accuracy: 44.44% | Highest Validation Accuracy: 92.00%\n","Training Years: 1991~2000 | Training Accuracy: 98.14% | Training OL Accuracy: 53.03%\n","Year: 2001~2002 Validation Accuracy: 92.80% | Only Long Accuracy: 46.20% | Highest Validation Accuracy: 92.00%\n","Year: 2003 | Test Accuracy: 88.89% | Only Long Accuracy: 54.37%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2001, 2002]_testYear:2003.pt\n","Year: 2001 | Validation Accuracy: 92.34% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 92.80%\n","Year: 2002 | Validation Accuracy: 90.87% | Only Long Accuracy: 44.44% | Highest Validation Accuracy: 92.80%\n","Training Years: 1991~2000 | Training Accuracy: 98.54% | Training OL Accuracy: 53.03%\n","Year: 2001~2002 Validation Accuracy: 91.60% | Only Long Accuracy: 46.20% | Highest Validation Accuracy: 92.80%\n","Year: 2003 | Test Accuracy: 84.92% | Only Long Accuracy: 54.37%\n","------------------------------------------------\n","Year: 2001 | Validation Accuracy: 94.35% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 92.80%\n","Year: 2002 | Validation Accuracy: 92.46% | Only Long Accuracy: 44.44% | Highest Validation Accuracy: 92.80%\n","Training Years: 1991~2000 | Training Accuracy: 99.21% | Training OL Accuracy: 53.03%\n","Year: 2001~2002 Validation Accuracy: 93.40% | Only Long Accuracy: 46.20% | Highest Validation Accuracy: 92.80%\n","Year: 2003 | Test Accuracy: 90.48% | Only Long Accuracy: 54.37%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2001, 2002]_testYear:2003.pt\n","Year: 2001 | Validation Accuracy: 95.16% | Only Long Accuracy: 47.98% | Highest Validation Accuracy: 93.40%\n","Year: 2002 | Validation Accuracy: 92.06% | Only Long Accuracy: 44.44% | Highest Validation Accuracy: 93.40%\n","Training Years: 1991~2000 | Training Accuracy: 99.64% | Training OL Accuracy: 53.03%\n","Year: 2001~2002 Validation Accuracy: 93.60% | Only Long Accuracy: 46.20% | Highest Validation Accuracy: 93.40%\n","Year: 2003 | Test Accuracy: 90.08% | Only Long Accuracy: 54.37%\n","------------------------------------------------\n","Model Saved at /content/drive/MyDrive/code/fintransformer/models/card_classification/[^SPX]_valYrs:2_testYrs1_reset:False_scale_run2/reset:False_valYear:[2001, 2002]_testYear:2003.pt\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 1/21 [03:55<1:18:34, 235.70s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-d9e182d88ce0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If result is different look at how batch_x_mark and batch_y_mark is passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcard_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-53-b4024b488a24>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_save_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecent_save_path\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mwindow_iteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_val_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_test_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maverage_val_accuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mhighest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mhighest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_val_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-b4024b488a24>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iteration, window_iteration, highest_acc)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m#print(f\"validation year: {year}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mbatches_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_x_mark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_y_mark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_hits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_ol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_x_mark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_y_mark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighest_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mvalidation_total_hits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_hits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mvalidation_total_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-b4024b488a24>\u001b[0m in \u001b[0;36meval_once\u001b[0;34m(self, batches_x, batches_y, batches_x_mark, batches_y_mark, year, highest_acc, train_acc, last)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbatch_y_direction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mtruth_direction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_direction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_direction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# If result is different look at how batch_x_mark and batch_y_mark is passed\n","card_trainer.run()"]},{"cell_type":"code","source":[],"metadata":{"id":"C04SSUfcP0KX"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["YsW8atyq_KNH","2Ll-r4SeAg1x","4mr8t8gAAecd","B3Q53Qu8lqvw"],"gpuType":"A100","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}